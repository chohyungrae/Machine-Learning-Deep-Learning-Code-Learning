{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27d5NvHbz7lV",
        "colab_type": "text"
      },
      "source": [
        "구글콜렙에서 열기:\n",
        "\n",
        "https://colab.research.google.com/github/chohyungrae/Machine-Learning-Deep-Learning-Code-Learning/blob/master/Softmax.ipynb\n",
        "\n",
        "참조사이트: https://yamalab.tistory.com/87"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDhDxTz8suUd",
        "colab_type": "text"
      },
      "source": [
        "softmax함수 수식\n",
        "![대체 텍스트](https://miro.medium.com/max/1400/0*tGSrq3hfKFBgKntB)\n",
        "일반적인 Neural Network의 activation function의 차이점 \n",
        "\n",
        "\n",
        "*   일반적:유닛 k의 출력 Zk는 입력 Uk로부터만 결정되는 것\n",
        "*   소프트맥수 함수;유닛 k의 출력 Zk는 Sum(Uk)로 결정된다.\n",
        "---\n",
        "시그모이드 함수는 로지스틱 함수의 한 케이스라 볼 수 있는데\n",
        "인풋의 개수에 따라 \n",
        "\n",
        "- input값이 하나밖에 없다면? sigmoid함수(logic함수)\n",
        "- input값이 여러개면? softmax함수\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFz2pSfuwv_z",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://miro.medium.com/max/1000/0*pK080h7eb0348DBm)\n",
        "j는 1부터 K까지의 범위를 가지고, \n",
        "\n",
        "z는 K차원 벡터\n",
        "\n",
        "예) 작은 숫자, 3과 5가 있다.\n",
        "\n",
        "숫자가 두 개이기 때문에, K는 2가 되고, z는 2차원벡터, \n",
        "\n",
        "K=[3, 5]가 된다.\n",
        "\n",
        "=> somftmax식에 대입함으로써, 두 숫자를 확률처럼 만들 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1eB2nncykTf",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/9919053B5B41E19529)\n",
        "\n",
        "\n",
        "*   Softmax는 정답 클래스를 one-hot encoding의 방법으로 학습시킨다.\n",
        "*   숫자(0.7, 0.2, 0.1)이 softmax를 통과하면 정답 데이터인 L은 확률값으로 바뀐다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ07mO79zfRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "- GD를 이용하여 Softmax regression 학습\n",
        "- 참고 : https://deepnotes.io/softmax-crossentropy\n",
        "\n",
        "'''\n",
        "\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, learning_rate=0.01, threshold=0.01, max_iterations=100000, verbose=False, reg_strength=1e-5):\n",
        "        self._learning_rate = learning_rate  # 학습 계수\n",
        "        self._max_iterations = max_iterations  # 반복 횟수\n",
        "        self._threshold = threshold  # 학습 중단 계수\n",
        "        self._verbose = verbose  # 중간 진행사항 출력 여부\n",
        "        self._reg_strength = reg_strength # 정규화 파라미터 계수\n",
        "\n",
        "    # theta(W) 계수들 return\n",
        "    def get_coeff(self):\n",
        "        return self._W\n",
        "\n",
        "    # softmax function\n",
        "    def softmax_func(self, x_data):\n",
        "        predictions = x_data - (x_data.max(axis=1).reshape([-1, 1]))\n",
        "        softmax = np.exp(predictions)\n",
        "        softmax /= softmax.sum(axis=1).reshape([-1, 1])\n",
        "        return softmax\n",
        "\n",
        "        # prediction result example\n",
        "        # [[0.01821127 0.24519181 0.73659691]\n",
        "        # [0.87279747 0.0791784  0.04802413]\n",
        "        # [0.05280815 0.86841135 0.0787805 ]]\n",
        "\n",
        "    # cost function 정의\n",
        "    def cost_func(self, softmax, y_data):\n",
        "        sample_size = y_data.shape[0]\n",
        "\n",
        "        # softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)]\n",
        "        # --> 해당 one-hot 의 class index * 해당 유닛의 출력을 각 row(1개의 input row)에 대해 계산\n",
        "        # --> (n, 1) 의 shape\n",
        "        cost = -np.log(softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)]).sum() \n",
        "        cost /= sample_size\n",
        "        cost += (self._reg_strength * (self._W**2).sum()) / 2\n",
        "        return cost\n",
        "\n",
        "    # gradient 계산 (regularized)\n",
        "    def gradient_func(self, softmax, x_data, y_data):\n",
        "        sample_size = y.shape[0]\n",
        "\n",
        "        # softmax cost function의 미분 결과는 pi−yi 이므로,\n",
        "        # softmax가 계산된 matrix에서, (해당 one-hot 의 class index * 해당 유닛)에 해당하는 유닛 위치에 -1을 더해줌.\n",
        "        softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)] -= 1\n",
        "        gradient = np.dot(x_data.transpose(), softmax) / sample_size\n",
        "        gradient += self._reg_strength * self._W\n",
        "        return gradient\n",
        "\n",
        "    # learning\n",
        "    def fit(self, x_data, y_data):\n",
        "        num_examples, num_features = np.shape(x_data)\n",
        "        num_classes = y.shape[1]\n",
        "\n",
        "        # 가중계수 초기화\n",
        "        self._W = np.random.randn(num_features, num_classes) / np.sqrt(num_features / 2)\n",
        "\n",
        "        for i in range(self._max_iterations):\n",
        "            \n",
        "            # y^ 계산\n",
        "            z = np.dot(x_data, self._W)\n",
        "            softmax = self.softmax_func(z)\n",
        "\n",
        "            # cost 함수\n",
        "            cost = self.cost_func(softmax, y_data)\n",
        "\n",
        "            # softmax 함수의 gradient (regularized)\n",
        "            gradient = self.gradient_func(softmax, x_data, y_data)\n",
        "\n",
        "            # gradient에 따라 theta 업데이트\n",
        "            self._W -= self._learning_rate * gradient\n",
        "\n",
        "            # 판정 임계값에 다다르면 학습 중단\n",
        "            if cost < self._threshold:\n",
        "                return False\n",
        "\n",
        "            # 100 iter 마다 cost 출력\n",
        "            if (self._verbose == True and i % 100 == 0):\n",
        "                print (\"Iter(Epoch): %s, Loss: %s\" % (i, cost))\n",
        "\n",
        "    # prediction\n",
        "    def predict(self, x_data):\n",
        "        return np.argmax(x_data.dot(self._W), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVt3VsG3D1pM",
        "colab_type": "text"
      },
      "source": [
        "## **동물의 특징에 따라 7가지로 분류 예제**\n",
        "참조사이트:https://www.youtube.com/watch?v=E-io76NlsqA          \n",
        "https://leechanho.tistory.com/23\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPp9JfKuIzeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "ed85b502-947c-4900-f68b-2d5f532ee5f1"
      },
      "source": [
        "pip install tensorflow==1.4.0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9f/be0165c6eefd841e6928e54d3d083fa174f92d640fdc52f73a33dc9c54d1/tensorflow-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (41.2MB)\n",
            "\u001b[K     |████████████████████████████████| 41.2MB 99kB/s \n",
            "\u001b[?25hCollecting enum34>=1.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (3.12.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (1.15.0)\n",
            "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/9f/5845c18f9df5e7ea638ecf3a272238f0e7671e454faa396b5188c6e6fc0a/tensorflow_tensorboard-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.4.0) (49.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.2.2)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.6MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=324b33d55e703e1219d0dea9d4e3f2f39f853188862b202b2a1a99249430088e\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: enum34, html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed bleach-1.5.0 enum34-1.1.10 html5lib-0.9999999 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5mtbHAFD-wD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a0cc744-a81a-4a9a-9cd6-298021b3d9c6"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 동물 데이터 불러오기\n",
        "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "# Y data가 0 ~ 6으로 7가지\n",
        "nb_classes = 7\n",
        "\n",
        "# placeholder\n",
        "X = tf.placeholder(tf.float32, shape=[None, 16])\n",
        "Y = tf.placeholder(tf.int32, shape=[None, 1])  # shape = (?, 1)\n",
        "\n",
        "# Y data를, one-hot으로 변경 : shape = (?, 1, 7)\n",
        "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
        "\n",
        "# one_hot을 통과하면 차원이 늘어나므로, reshape로 줄여주기 : shape = (?, 7)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([16, nb_classes]), name=\"weight\")\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name=\"bias\")\n",
        "\n",
        "# Hypothesis : softmax function 사용\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits))\n",
        "logits = tf.matmul(X, W) + b\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "\n",
        "# cost/loss function : cross entropy\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
        "cost = tf.reduce_mean(cost_i)\n",
        "\n",
        "# Minimize : Gradient Descent 사용\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# argmax() : [0.1, 0.3, 0.5]의 argmax는 1로 가장 큰 값의 index 출력\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# 세션 시작\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(2001):\n",
        "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "            print(step, sess.run([cost, accuracy], feed_dict={X: x_data, Y: y_data}))\n",
        "            # 2000 [0.053728174, 1.0]\n",
        "\n",
        "    # Predict Test\n",
        "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
        "    # y_data.flatten() : 다차원 배열을 1차원 배열로 쭉 펴준다.\n",
        "    # zip : pred와 y_data.flatten() 2개의 배열을 하나로 묶어서 p, y로 넘겨줌\n",
        "    for p, y in zip(pred, y_data.flatten()):\n",
        "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n",
        "        # [True] Prediction: 6 True Y: 6\n",
        "        # [True] Prediction: 1 True Y: 1\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [2.956933, 0.5346535]\n",
            "100 [0.7361294, 0.77227724]\n",
            "200 [0.43827242, 0.9009901]\n",
            "300 [0.31879058, 0.9207921]\n",
            "400 [0.25159103, 0.95049506]\n",
            "500 [0.20797017, 0.95049506]\n",
            "600 [0.17733647, 0.97029704]\n",
            "700 [0.15464145, 0.97029704]\n",
            "800 [0.13711333, 0.97029704]\n",
            "900 [0.12312869, 0.97029704]\n",
            "1000 [0.11168861, 0.97029704]\n",
            "1100 [0.10214683, 0.980198]\n",
            "1200 [0.094064854, 0.980198]\n",
            "1300 [0.087132744, 1.0]\n",
            "1400 [0.08112388, 1.0]\n",
            "1500 [0.07586803, 1.0]\n",
            "1600 [0.0712346, 1.0]\n",
            "1700 [0.06712143, 1.0]\n",
            "1800 [0.063447416, 1.0]\n",
            "1900 [0.060147308, 1.0]\n",
            "2000 [0.057168107, 1.0]\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igFGQuBMGTdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}