{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPX8Vat+nguXN5JlD8zeIV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chohyungrae/Machine-Learning-Deep-Learning-Code-Learning/blob/master/Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27d5NvHbz7lV",
        "colab_type": "text"
      },
      "source": [
        "참조사이트: https://yamalab.tistory.com/87"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDhDxTz8suUd",
        "colab_type": "text"
      },
      "source": [
        "softmax함수 수식\n",
        "![대체 텍스트](https://miro.medium.com/max/1400/0*tGSrq3hfKFBgKntB)\n",
        "일반적인 Neural Network의 activation function의 차이점 \n",
        "\n",
        "\n",
        "*   일반적:유닛 k의 출력 Zk는 입력 Uk로부터만 결정되는 것\n",
        "*   소프트맥수 함수;유닛 k의 출력 Zk는 Sum(Uk)로 결정된다.\n",
        "---\n",
        "시그모이드 함수는 로지스틱 함수의 한 케이스라 볼 수 있는데\n",
        "인풋의 개수에 따라 \n",
        "\n",
        "- input값이 하나밖에 없다면? sigmoid함수(logic함수)\n",
        "- input값이 여러개면? softmax함수\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFz2pSfuwv_z",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://miro.medium.com/max/1000/0*pK080h7eb0348DBm)\n",
        "j는 1부터 K까지의 범위를 가지고, \n",
        "\n",
        "z는 K차원 벡터\n",
        "\n",
        "예) 작은 숫자, 3과 5가 있다.\n",
        "\n",
        "숫자가 두 개이기 때문에, K는 2가 되고, z는 2차원벡터, \n",
        "\n",
        "K=[3, 5]가 된다.\n",
        "\n",
        "=> somftmax식에 대입함으로써, 두 숫자를 확률처럼 만들 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1eB2nncykTf",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/9919053B5B41E19529)\n",
        "\n",
        "\n",
        "*   Softmax는 정답 클래스를 one-hot encoding의 방법으로 학습시킨다.\n",
        "*   숫자(0.7, 0.2, 0.1)이 softmax를 통과하면 정답 데이터인 L은 확률값으로 바뀐다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ07mO79zfRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "- GD를 이용하여 Softmax regression 학습\n",
        "- 참고 : https://deepnotes.io/softmax-crossentropy\n",
        "\n",
        "'''\n",
        "\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, learning_rate=0.01, threshold=0.01, max_iterations=100000, verbose=False, reg_strength=1e-5):\n",
        "        self._learning_rate = learning_rate  # 학습 계수\n",
        "        self._max_iterations = max_iterations  # 반복 횟수\n",
        "        self._threshold = threshold  # 학습 중단 계수\n",
        "        self._verbose = verbose  # 중간 진행사항 출력 여부\n",
        "        self._reg_strength = reg_strength # 정규화 파라미터 계수\n",
        "\n",
        "    # theta(W) 계수들 return\n",
        "    def get_coeff(self):\n",
        "        return self._W\n",
        "\n",
        "    # softmax function\n",
        "    def softmax_func(self, x_data):\n",
        "        predictions = x_data - (x_data.max(axis=1).reshape([-1, 1]))\n",
        "        softmax = np.exp(predictions)\n",
        "        softmax /= softmax.sum(axis=1).reshape([-1, 1])\n",
        "        return softmax\n",
        "\n",
        "        # prediction result example\n",
        "        # [[0.01821127 0.24519181 0.73659691]\n",
        "        # [0.87279747 0.0791784  0.04802413]\n",
        "        # [0.05280815 0.86841135 0.0787805 ]]\n",
        "\n",
        "    # cost function 정의\n",
        "    def cost_func(self, softmax, y_data):\n",
        "        sample_size = y_data.shape[0]\n",
        "\n",
        "        # softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)]\n",
        "        # --> 해당 one-hot 의 class index * 해당 유닛의 출력을 각 row(1개의 input row)에 대해 계산\n",
        "        # --> (n, 1) 의 shape\n",
        "        cost = -np.log(softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)]).sum() \n",
        "        cost /= sample_size\n",
        "        cost += (self._reg_strength * (self._W**2).sum()) / 2\n",
        "        return cost\n",
        "\n",
        "    # gradient 계산 (regularized)\n",
        "    def gradient_func(self, softmax, x_data, y_data):\n",
        "        sample_size = y.shape[0]\n",
        "\n",
        "        # softmax cost function의 미분 결과는 pi−yi 이므로,\n",
        "        # softmax가 계산된 matrix에서, (해당 one-hot 의 class index * 해당 유닛)에 해당하는 유닛 위치에 -1을 더해줌.\n",
        "        softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)] -= 1\n",
        "        gradient = np.dot(x_data.transpose(), softmax) / sample_size\n",
        "        gradient += self._reg_strength * self._W\n",
        "        return gradient\n",
        "\n",
        "    # learning\n",
        "    def fit(self, x_data, y_data):\n",
        "        num_examples, num_features = np.shape(x_data)\n",
        "        num_classes = y.shape[1]\n",
        "\n",
        "        # 가중계수 초기화\n",
        "        self._W = np.random.randn(num_features, num_classes) / np.sqrt(num_features / 2)\n",
        "\n",
        "        for i in range(self._max_iterations):\n",
        "            \n",
        "            # y^ 계산\n",
        "            z = np.dot(x_data, self._W)\n",
        "            softmax = self.softmax_func(z)\n",
        "\n",
        "            # cost 함수\n",
        "            cost = self.cost_func(softmax, y_data)\n",
        "\n",
        "            # softmax 함수의 gradient (regularized)\n",
        "            gradient = self.gradient_func(softmax, x_data, y_data)\n",
        "\n",
        "            # gradient에 따라 theta 업데이트\n",
        "            self._W -= self._learning_rate * gradient\n",
        "\n",
        "            # 판정 임계값에 다다르면 학습 중단\n",
        "            if cost < self._threshold:\n",
        "                return False\n",
        "\n",
        "            # 100 iter 마다 cost 출력\n",
        "            if (self._verbose == True and i % 100 == 0):\n",
        "                print (\"Iter(Epoch): %s, Loss: %s\" % (i, cost))\n",
        "\n",
        "    # prediction\n",
        "    def predict(self, x_data):\n",
        "        return np.argmax(x_data.dot(self._W), 1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCRZFtYg0Eej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}