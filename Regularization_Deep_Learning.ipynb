{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization Deep Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOa3rLLjiIfo80APeIyV1uJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chohyungrae/Machine-Learning-Deep-Learning-Code-Learning/blob/master/Regularization_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNVvgZAQgT8w",
        "colab_type": "text"
      },
      "source": [
        "### **Regularization Techniques in Deep Learning**\n",
        "\n",
        "- 정규화는 학습 알고리즘을 약간 수정하여 모델이 일반화되는 기법.\n",
        "\n",
        "- 보이지 않는 데이터에 대한 모델의 성능도 향상시키게됨.\n",
        "\n",
        "> 정규화의 대표적인 기법)\n",
        "\n",
        "**- L2 & L1 regularization**\n",
        "\n",
        "Cost function = Loss (say, binary cross entropy) + Regularization term\n",
        "위식은\n",
        "이 정규화 항의 추가로 인해, \n",
        "\n",
        "가중치 행렬의 값은 더 작은 가중치 행렬을 갖는 신경망으로 더 간단한 모델이 되어 가중치는 감소됨. \n",
        "\n",
        "따라서 overfitting을 상당히 줄일 수 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eNLQ27qikFI",
        "colab_type": "text"
      },
      "source": [
        "# **L2**\n",
        "![대체 텍스트](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-1.59.54-AM.png)\n",
        "- 람다 는 정규화 매개 변수\n",
        "- 더 나은 결과를 위해 값이 최적화 된 하이퍼 파라미터\n",
        "- L2 정규화는 가중치가 0 (정확히 0이 아님)을 향하여 쇠퇴 하게하므로 가중치 감소 라고 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5BOhfP_jWNJ",
        "colab_type": "text"
      },
      "source": [
        "## **L1**\n",
        "![대체 텍스트](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-1.59.57-AM.png)\n",
        "- 가중치의 절대 값에 불이익을 준다.\n",
        "- L2와 달리 가중치는 완전히 0으로 줄어들 수 있다. \n",
        "- 따라서 L1은 모델을 압축하려고 할 때 매우 유용하다. \n",
        "- 그렇지 않으면 일반적으로 L2를 선호한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXUCEOu7iwvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# L2 정규화 샘플 코드\n",
        "from keras import regularizers\n",
        "model.add(Dense(64, input_dim=64,\n",
        "                kernel_regularizer=regularizers.l2(0.01)\n",
        "\n",
        "#0.01 값은 정규화 매개 변수, 즉 람다의 값\n",
        "#람다값은 더 최적화해야 한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7nDDqOglhbv",
        "colab_type": "text"
      },
      "source": [
        "## **Dropout**\n",
        "- 정규화 방법중 하나\n",
        "- 반복 할 때마다 일부 노드를 임의로 선택하고 모든 수신 및 발신 연결과 함께 노드를 제거\n",
        "- 앙상블 기법\n",
        "- 드롭 아웃은 입력 레이어뿐만 아니라 숨겨진 레이어에도 적용이 가능\n",
        "\n",
        "![대체 텍스트](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLJ27-yGmHhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#드롭아웃\n",
        "#떨어질 확률로 0.25를 정의.\n",
        "#그리드 검색 방법을 사용하여 더 나은 결과를 얻을 수 있도록 더 조정할 수 있다.\n",
        "from keras.layers.core import Dropout\n",
        "\n",
        "model = Sequential([\n",
        " Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
        " Dropout(0.25),\n",
        "\n",
        "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
        " ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pfcrGwmm4rg",
        "colab_type": "text"
      },
      "source": [
        "## **Data Augmentation**\n",
        "- 정규화 방법중 하나\n",
        "- 과적 합을 줄이는 가장 간단한 방법은 훈련 데이터의 크기를 늘리는 것\n",
        "- 방법적으로 이미지를 회전, 뒤집기, 크기 조절, 이동 등을 통해 훈련 데이터의 크기를 늘리는 것\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-Yi8SrnSwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#케라스 Data Augmentation 코드\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(horizontal flip=True)\n",
        "datagen.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEJ-zIOXocwV",
        "colab_type": "text"
      },
      "source": [
        "## **Early stopping(조기중지)**\n",
        "- 조기 중지는 훈련 세트의 한 부분을 검증 세트로 유지하는 일종의 교차 검증 전략\n",
        "- 검증 세트의 성능이 악화되고 있음을 발견하면 즉시 모델 교육을 중단\n",
        "\n",
        "![대체 텍스트](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-12.31.56-AM.png)\n",
        "\n",
        "- 위의 이미지에서 모델이 훈련 데이터에 과적 합하기 시작한 이후 점선에서 훈련을 중단한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNwcRdYgpzFF",
        "colab_type": "text"
      },
      "source": [
        "콜백함수를 통해\n",
        "- 모든 훈련 배치 후 TensorBoard 로그를 작성하여 메트릭 모니터링\n",
        "- 주기적으로 모델을 디스크에 저장\n",
        "- 조기 정지\n",
        "- 훈련 중 모델의 내부 상태 및 통계를 볼 수 있다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fmjj-wEqPPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**< 내장 fit()루프 를 통한 콜백 사용>**\n",
        "#키워드 인수로 콜백 목록을 모델 callbacks의 .fit()메소드에 전달\n",
        "#그런 다음 교육의 각 단계에서 콜백 관련 메소드가 호출\n",
        "\n",
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]\n",
        "model.fit(dataset, epochs=10, callbacks=my_callbacks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZgmC9LKqajh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d24b399c-55a0-4711-b798-b8a39ce2c528"
      },
      "source": [
        "#콜백함수\n",
        "'''\n",
        "위의 이미지에서 점선 후, \n",
        "각각의 에포크는 더 높은 값의 검증 에러를 야기 할 것이다. \n",
        "따라서 점선 뒤에 5개의 에포크(Patience가 5와 같기 때문에)가 더 이상 개선되지 않기 때문에 모델이 중지된다.\n",
        "\n",
        "참고 : 5 eporch( 일반적으로 patience로 정의 된 값 ) 후에 \n",
        "모델이 다시 개선되기 시작하고 유효성 검증 오류도 감소하기 시작할 수 있다. \n",
        "따라서 이 하이퍼 파라미터를 조정하는 동안 각별히주의해야한다.\n",
        "'''\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "EarlyStopping(monitor='val_err', patience=5)\n",
        "#monitor 는 모니터링해야 할 수량을 나타내고 \n",
        "#val_err 은 유효성 검사 오류를 나타냄\n",
        "#Patience은 훈련이 중단 된 후 더 이상 개선되지 않은 에포크의 수"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.EarlyStopping at 0x7f3e6bb896a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwj9yjDlrHUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}